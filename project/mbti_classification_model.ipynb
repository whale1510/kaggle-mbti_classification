{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## project : MBTI classification with LIWC data / skku 기계학습 수업\n",
        "info : MBTI를 공개한 사람들이 올린 텍스트의 LIWC(Linguistic Inquiry and Word Count) 분석값을 바탕으로\n",
        "T(사고형)과 F(감정형)을 분류하는 프로젝트 \n",
        "name : 조병웅  \n",
        "model : 해당 프로젝트에는 logistic 회귀 scratch 코드 모델을 사용.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 캐글 데이터 로딩 및 라이브러리 호출\n",
        "캐글 주소 -> https://www.kaggle.com/competitions/skku-2023-1-machine-learning-second-project/overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhGqPekarSBk",
        "outputId": "d870cd94-4fa2-4f3f-e3ac-abe97dd45da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vhhJocAm8uq"
      },
      "source": [
        "## data 전처리 부분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7VS8fIEvsWyK"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3l4MQ21GQxRg"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "shdPto3UQ7WU",
        "outputId": "a70aa201-253e-4979-f45f-ad8606eb5007"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f351dccd-178b-4df0-b45d-6175fbd0f284\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>WPS</th>\n",
              "      <th>Sixltr</th>\n",
              "      <th>Dic</th>\n",
              "      <th>function</th>\n",
              "      <th>...</th>\n",
              "      <th>Comma</th>\n",
              "      <th>Colon</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Quote</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>Parenth</th>\n",
              "      <th>OtherP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1288</td>\n",
              "      <td>38.60</td>\n",
              "      <td>40.80</td>\n",
              "      <td>64.89</td>\n",
              "      <td>31.05</td>\n",
              "      <td>21.83</td>\n",
              "      <td>18.09</td>\n",
              "      <td>82.92</td>\n",
              "      <td>51.01</td>\n",
              "      <td>...</td>\n",
              "      <td>7.30</td>\n",
              "      <td>2.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.16</td>\n",
              "      <td>15.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>953</td>\n",
              "      <td>21.92</td>\n",
              "      <td>23.14</td>\n",
              "      <td>49.21</td>\n",
              "      <td>34.90</td>\n",
              "      <td>16.72</td>\n",
              "      <td>15.22</td>\n",
              "      <td>76.39</td>\n",
              "      <td>47.85</td>\n",
              "      <td>...</td>\n",
              "      <td>3.78</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.36</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.56</td>\n",
              "      <td>2.31</td>\n",
              "      <td>18.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1388</td>\n",
              "      <td>36.27</td>\n",
              "      <td>26.30</td>\n",
              "      <td>85.07</td>\n",
              "      <td>62.65</td>\n",
              "      <td>13.35</td>\n",
              "      <td>15.06</td>\n",
              "      <td>83.93</td>\n",
              "      <td>51.59</td>\n",
              "      <td>...</td>\n",
              "      <td>6.34</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.36</td>\n",
              "      <td>12.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1294</td>\n",
              "      <td>42.36</td>\n",
              "      <td>60.67</td>\n",
              "      <td>26.94</td>\n",
              "      <td>99.00</td>\n",
              "      <td>23.53</td>\n",
              "      <td>15.69</td>\n",
              "      <td>84.31</td>\n",
              "      <td>49.77</td>\n",
              "      <td>...</td>\n",
              "      <td>4.25</td>\n",
              "      <td>2.78</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.63</td>\n",
              "      <td>0.77</td>\n",
              "      <td>15.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1377</td>\n",
              "      <td>19.83</td>\n",
              "      <td>49.42</td>\n",
              "      <td>53.79</td>\n",
              "      <td>53.05</td>\n",
              "      <td>105.92</td>\n",
              "      <td>14.16</td>\n",
              "      <td>88.24</td>\n",
              "      <td>57.52</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.22</td>\n",
              "      <td>2.54</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.73</td>\n",
              "      <td>11.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1730</th>\n",
              "      <td>1730</td>\n",
              "      <td>797</td>\n",
              "      <td>47.84</td>\n",
              "      <td>33.93</td>\n",
              "      <td>72.52</td>\n",
              "      <td>61.46</td>\n",
              "      <td>15.33</td>\n",
              "      <td>15.31</td>\n",
              "      <td>86.20</td>\n",
              "      <td>51.94</td>\n",
              "      <td>...</td>\n",
              "      <td>4.02</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>16.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1731</th>\n",
              "      <td>1731</td>\n",
              "      <td>1231</td>\n",
              "      <td>37.88</td>\n",
              "      <td>24.23</td>\n",
              "      <td>76.55</td>\n",
              "      <td>31.30</td>\n",
              "      <td>21.98</td>\n",
              "      <td>19.25</td>\n",
              "      <td>84.81</td>\n",
              "      <td>53.94</td>\n",
              "      <td>...</td>\n",
              "      <td>3.74</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.08</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.08</td>\n",
              "      <td>3.98</td>\n",
              "      <td>1.71</td>\n",
              "      <td>13.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>1732</td>\n",
              "      <td>996</td>\n",
              "      <td>51.62</td>\n",
              "      <td>50.80</td>\n",
              "      <td>51.90</td>\n",
              "      <td>46.09</td>\n",
              "      <td>23.16</td>\n",
              "      <td>16.77</td>\n",
              "      <td>81.12</td>\n",
              "      <td>49.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.60</td>\n",
              "      <td>2.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.51</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1733</th>\n",
              "      <td>1733</td>\n",
              "      <td>1610</td>\n",
              "      <td>19.15</td>\n",
              "      <td>23.00</td>\n",
              "      <td>79.30</td>\n",
              "      <td>91.92</td>\n",
              "      <td>21.76</td>\n",
              "      <td>15.90</td>\n",
              "      <td>84.78</td>\n",
              "      <td>54.10</td>\n",
              "      <td>...</td>\n",
              "      <td>4.72</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.68</td>\n",
              "      <td>2.98</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.40</td>\n",
              "      <td>1.55</td>\n",
              "      <td>10.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1734</th>\n",
              "      <td>1734</td>\n",
              "      <td>1462</td>\n",
              "      <td>34.47</td>\n",
              "      <td>69.36</td>\n",
              "      <td>48.17</td>\n",
              "      <td>60.78</td>\n",
              "      <td>20.89</td>\n",
              "      <td>17.17</td>\n",
              "      <td>91.24</td>\n",
              "      <td>58.62</td>\n",
              "      <td>...</td>\n",
              "      <td>4.45</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.48</td>\n",
              "      <td>10.53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1735 rows × 94 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f351dccd-178b-4df0-b45d-6175fbd0f284')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f351dccd-178b-4df0-b45d-6175fbd0f284 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f351dccd-178b-4df0-b45d-6175fbd0f284');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id    WC  Analytic  Clout  Authentic   Tone     WPS  Sixltr    Dic  \\\n",
              "0        0  1288     38.60  40.80      64.89  31.05   21.83   18.09  82.92   \n",
              "1        1   953     21.92  23.14      49.21  34.90   16.72   15.22  76.39   \n",
              "2        2  1388     36.27  26.30      85.07  62.65   13.35   15.06  83.93   \n",
              "3        3  1294     42.36  60.67      26.94  99.00   23.53   15.69  84.31   \n",
              "4        4  1377     19.83  49.42      53.79  53.05  105.92   14.16  88.24   \n",
              "...    ...   ...       ...    ...        ...    ...     ...     ...    ...   \n",
              "1730  1730   797     47.84  33.93      72.52  61.46   15.33   15.31  86.20   \n",
              "1731  1731  1231     37.88  24.23      76.55  31.30   21.98   19.25  84.81   \n",
              "1732  1732   996     51.62  50.80      51.90  46.09   23.16   16.77  81.12   \n",
              "1733  1733  1610     19.15  23.00      79.30  91.92   21.76   15.90  84.78   \n",
              "1734  1734  1462     34.47  69.36      48.17  60.78   20.89   17.17  91.24   \n",
              "\n",
              "      function  ...  Comma  Colon  SemiC  QMark  Exclam  Dash  Quote  Apostro  \\\n",
              "0        51.01  ...   7.30   2.02   0.00   1.16    0.47  0.39   0.00     3.11   \n",
              "1        47.85  ...   3.78   0.63   0.00   1.05    1.36  1.05   0.00     5.56   \n",
              "2        51.59  ...   6.34   0.72   0.00   0.58    0.22  0.79   0.00     3.75   \n",
              "3        49.77  ...   4.25   2.78   1.24   0.77    1.39  0.85   0.00     3.63   \n",
              "4        57.52  ...   0.29   0.58   0.00   0.22    2.54  0.65   0.00     0.80   \n",
              "...        ...  ...    ...    ...    ...    ...     ...   ...    ...      ...   \n",
              "1730     51.94  ...   4.02   0.50   0.00   1.38    0.38  1.13   0.00     0.75   \n",
              "1731     53.94  ...   3.74   0.49   0.08   0.73    0.08  1.22   0.08     3.98   \n",
              "1732     49.50  ...   0.70   0.00   0.00   0.90    0.60  2.41   0.00     2.51   \n",
              "1733     54.10  ...   4.72   1.49   0.00   0.68    2.98  0.99   0.00     6.40   \n",
              "1734     58.62  ...   4.45   0.75   0.00   1.03    0.07  0.55   0.00     1.92   \n",
              "\n",
              "      Parenth  OtherP  \n",
              "0        0.16   15.99  \n",
              "1        2.31   18.26  \n",
              "2        0.36   12.10  \n",
              "3        0.77   15.53  \n",
              "4        0.73   11.62  \n",
              "...       ...     ...  \n",
              "1730     0.50   16.94  \n",
              "1731     1.71   13.24  \n",
              "1732     0.00   20.18  \n",
              "1733     1.55   10.99  \n",
              "1734     0.48   10.53  \n",
              "\n",
              "[1735 rows x 94 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Qr1iUWYMr7jY",
        "outputId": "1b56c3e1-480d-40b3-ecb4-5b90ffe27b5d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-992c399c-f2f8-45d9-acfc-d1f0f07aa5a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>WPS</th>\n",
              "      <th>Sixltr</th>\n",
              "      <th>Dic</th>\n",
              "      <th>function</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>...</th>\n",
              "      <th>Colon</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Quote</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>Parenth</th>\n",
              "      <th>OtherP</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1078</td>\n",
              "      <td>51.81</td>\n",
              "      <td>51.86</td>\n",
              "      <td>38.80</td>\n",
              "      <td>55.50</td>\n",
              "      <td>25.07</td>\n",
              "      <td>21.80</td>\n",
              "      <td>74.86</td>\n",
              "      <td>43.88</td>\n",
              "      <td>14.66</td>\n",
              "      <td>...</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.62</td>\n",
              "      <td>1.67</td>\n",
              "      <td>19.20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>883</td>\n",
              "      <td>12.54</td>\n",
              "      <td>52.27</td>\n",
              "      <td>60.12</td>\n",
              "      <td>88.84</td>\n",
              "      <td>19.20</td>\n",
              "      <td>14.04</td>\n",
              "      <td>92.07</td>\n",
              "      <td>60.82</td>\n",
              "      <td>20.84</td>\n",
              "      <td>...</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.12</td>\n",
              "      <td>0.45</td>\n",
              "      <td>19.71</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1593</td>\n",
              "      <td>63.46</td>\n",
              "      <td>62.10</td>\n",
              "      <td>37.47</td>\n",
              "      <td>57.83</td>\n",
              "      <td>28.96</td>\n",
              "      <td>14.69</td>\n",
              "      <td>83.18</td>\n",
              "      <td>57.06</td>\n",
              "      <td>17.01</td>\n",
              "      <td>...</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.48</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>970</td>\n",
              "      <td>36.80</td>\n",
              "      <td>24.16</td>\n",
              "      <td>72.78</td>\n",
              "      <td>92.82</td>\n",
              "      <td>13.29</td>\n",
              "      <td>14.74</td>\n",
              "      <td>91.75</td>\n",
              "      <td>59.07</td>\n",
              "      <td>19.59</td>\n",
              "      <td>...</td>\n",
              "      <td>2.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.43</td>\n",
              "      <td>0.41</td>\n",
              "      <td>11.44</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1339</td>\n",
              "      <td>39.78</td>\n",
              "      <td>47.91</td>\n",
              "      <td>38.15</td>\n",
              "      <td>95.89</td>\n",
              "      <td>28.49</td>\n",
              "      <td>17.63</td>\n",
              "      <td>86.41</td>\n",
              "      <td>52.73</td>\n",
              "      <td>17.48</td>\n",
              "      <td>...</td>\n",
              "      <td>5.30</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.42</td>\n",
              "      <td>1.49</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.57</td>\n",
              "      <td>0.97</td>\n",
              "      <td>12.99</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6935</th>\n",
              "      <td>1459</td>\n",
              "      <td>37.27</td>\n",
              "      <td>13.04</td>\n",
              "      <td>94.70</td>\n",
              "      <td>88.49</td>\n",
              "      <td>44.21</td>\n",
              "      <td>14.05</td>\n",
              "      <td>86.36</td>\n",
              "      <td>54.83</td>\n",
              "      <td>17.96</td>\n",
              "      <td>...</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.40</td>\n",
              "      <td>2.26</td>\n",
              "      <td>11.79</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6936</th>\n",
              "      <td>1654</td>\n",
              "      <td>22.17</td>\n",
              "      <td>16.36</td>\n",
              "      <td>87.19</td>\n",
              "      <td>64.73</td>\n",
              "      <td>23.63</td>\n",
              "      <td>14.51</td>\n",
              "      <td>90.57</td>\n",
              "      <td>61.91</td>\n",
              "      <td>20.19</td>\n",
              "      <td>...</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.41</td>\n",
              "      <td>0.12</td>\n",
              "      <td>9.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6937</th>\n",
              "      <td>1673</td>\n",
              "      <td>42.46</td>\n",
              "      <td>30.78</td>\n",
              "      <td>85.61</td>\n",
              "      <td>76.57</td>\n",
              "      <td>15.35</td>\n",
              "      <td>17.27</td>\n",
              "      <td>87.99</td>\n",
              "      <td>57.14</td>\n",
              "      <td>19.13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>9.38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6938</th>\n",
              "      <td>1433</td>\n",
              "      <td>25.87</td>\n",
              "      <td>55.54</td>\n",
              "      <td>30.88</td>\n",
              "      <td>95.40</td>\n",
              "      <td>11.65</td>\n",
              "      <td>15.00</td>\n",
              "      <td>88.35</td>\n",
              "      <td>55.97</td>\n",
              "      <td>17.10</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.58</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.37</td>\n",
              "      <td>0.98</td>\n",
              "      <td>11.30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6939</th>\n",
              "      <td>1495</td>\n",
              "      <td>39.29</td>\n",
              "      <td>16.94</td>\n",
              "      <td>85.79</td>\n",
              "      <td>69.88</td>\n",
              "      <td>93.44</td>\n",
              "      <td>13.71</td>\n",
              "      <td>91.04</td>\n",
              "      <td>58.60</td>\n",
              "      <td>16.66</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.74</td>\n",
              "      <td>10.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6940 rows × 94 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-992c399c-f2f8-45d9-acfc-d1f0f07aa5a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-992c399c-f2f8-45d9-acfc-d1f0f07aa5a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-992c399c-f2f8-45d9-acfc-d1f0f07aa5a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        WC  Analytic  Clout  Authentic   Tone    WPS  Sixltr    Dic  function  \\\n",
              "0     1078     51.81  51.86      38.80  55.50  25.07   21.80  74.86     43.88   \n",
              "1      883     12.54  52.27      60.12  88.84  19.20   14.04  92.07     60.82   \n",
              "2     1593     63.46  62.10      37.47  57.83  28.96   14.69  83.18     57.06   \n",
              "3      970     36.80  24.16      72.78  92.82  13.29   14.74  91.75     59.07   \n",
              "4     1339     39.78  47.91      38.15  95.89  28.49   17.63  86.41     52.73   \n",
              "...    ...       ...    ...        ...    ...    ...     ...    ...       ...   \n",
              "6935  1459     37.27  13.04      94.70  88.49  44.21   14.05  86.36     54.83   \n",
              "6936  1654     22.17  16.36      87.19  64.73  23.63   14.51  90.57     61.91   \n",
              "6937  1673     42.46  30.78      85.61  76.57  15.35   17.27  87.99     57.14   \n",
              "6938  1433     25.87  55.54      30.88  95.40  11.65   15.00  88.35     55.97   \n",
              "6939  1495     39.29  16.94      85.79  69.88  93.44   13.71  91.04     58.60   \n",
              "\n",
              "      pronoun  ...  Colon  SemiC  QMark  Exclam  Dash  Quote  Apostro  \\\n",
              "0       14.66  ...   0.37   0.19   1.48    0.00  1.95    0.0     3.62   \n",
              "1       20.84  ...   0.34   0.00   1.70    0.23  0.91    0.0     6.12   \n",
              "2       17.01  ...   0.06   0.00   0.13    0.06  1.00    0.0     2.20   \n",
              "3       19.59  ...   2.27   0.00   0.82    1.44  0.10    0.0     4.43   \n",
              "4       17.48  ...   5.30   0.07   1.42    1.49  1.79    0.0     1.57   \n",
              "...       ...  ...    ...    ...    ...     ...   ...    ...      ...   \n",
              "6935    17.96  ...   0.96   0.00   0.55    0.34  1.64    0.0     2.40   \n",
              "6936    20.19  ...   0.24   0.00   0.18    0.12  0.42    0.0     6.41   \n",
              "6937    19.13  ...   0.24   0.06   0.24    1.49  0.30    0.0     2.39   \n",
              "6938    17.10  ...   0.21   0.07   1.05    2.58  0.70    0.0     2.37   \n",
              "6939    16.66  ...   0.80   0.00   0.13    0.07  0.74    0.0     1.54   \n",
              "\n",
              "      Parenth  OtherP  label  \n",
              "0        1.67   19.20      0  \n",
              "1        0.45   19.71      0  \n",
              "2        0.00   10.48      0  \n",
              "3        0.41   11.44      1  \n",
              "4        0.97   12.99      1  \n",
              "...       ...     ...    ...  \n",
              "6935     2.26   11.79      1  \n",
              "6936     0.12    9.67      1  \n",
              "6937     0.66    9.38      1  \n",
              "6938     0.98   11.30      1  \n",
              "6939     0.74   10.57      1  \n",
              "\n",
              "[6940 rows x 94 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imS8p3WbLMmv",
        "outputId": "5fe2b4dc-1c00-4c27-a73f-b55ed8e92803"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-761b05010f69>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[i] = np.where((data[i] < m)| (data[i] > M), np.nan, data[i])\n",
            "<ipython-input-12-761b05010f69>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[i] = np.where((data[i] < m)| (data[i] > M), np.nan, data[i])\n",
            "<ipython-input-12-761b05010f69>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[i] = np.where((data[i] < m)| (data[i] > M), np.nan, data[i])\n",
            "<ipython-input-12-761b05010f69>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[i] = np.where((data[i] < m)| (data[i] > M), np.nan, data[i])\n",
            "<ipython-input-12-761b05010f69>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[i] = np.where((data[i] < m)| (data[i] > M), np.nan, data[i])\n",
            "<ipython-input-12-761b05010f69>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[i] = np.where((data[i] < m)| (data[i] > M), np.nan, data[i])\n"
          ]
        }
      ],
      "source": [
        "#이상치 제거\n",
        "for i in data.columns:\n",
        "  if i == 'label':\n",
        "    pass\n",
        "  else:\n",
        "    p25 = data[i].quantile(0.25)\n",
        "    p75 = data[i].quantile(0.75)\n",
        "    iqr = p75- p25 \n",
        "    m = p25 - 1.5*iqr\n",
        "    M = p75 + 1.5*iqr\n",
        "    data[i] = np.where((data[i] < m)| (data[i] > M), np.nan, data[i])\n",
        "    data = data.dropna(axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akj33_sQFnFN"
      },
      "source": [
        "## 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5s4THCf5FoTB"
      },
      "outputs": [],
      "source": [
        "#로지스틱 회귀 참고.\n",
        "__author__ = \"조병웅\"\n",
        "__id__ = \"2019312570\"\n",
        "\n",
        "import numpy as np\n",
        "class LogisticRegression:\n",
        "    def __init__(self, max_iter=500, penalty=\"l2\", initialize = \"one\", random_seed = 1213):\n",
        "        \"\"\"\n",
        "        이 함수는 수정할 필요가 없습니다.\n",
        "        \"\"\"\n",
        "        self.author = __author__\n",
        "        self.id = __id__\n",
        "        \n",
        "        self.max_iter = max_iter\n",
        "        self.penalty = penalty\n",
        "        self.initialize = initialize\n",
        "        self.random_seed = random_seed\n",
        "        self.lr = 0.01\n",
        "        self.lamb = 0.01\n",
        "        np.random.seed(self.random_seed)\n",
        "\n",
        "        if self.penalty not in [\"l1\", \"l2\"]:\n",
        "            raise ValueError(\"Penalty must be l1 or l2\")\n",
        "\n",
        "        if self.initialize not in [\"one\", \"LeCun\", \"random\"]:\n",
        "            raise ValueError(\"Only [LeCun, One, random] Initialization supported\")\n",
        "\n",
        "            \n",
        "    def activation(self, z):\n",
        "\n",
        "        a = 1/(1+np.exp(-z))\n",
        "        return a\n",
        "\n",
        "\n",
        "    def fwpass(self, x):\n",
        "\n",
        "        z = np.dot(x, self.w) + self.b\n",
        "\n",
        "        z = self.activation(z)              \n",
        "        return z\n",
        "\n",
        "\n",
        "    def bwpass(self, x, err): \n",
        "\n",
        "        if self.penalty == \"l1\":\n",
        "            sum = 0 \n",
        "            for i in range(len(self.w)):\n",
        "                sum += abs(self.w[i]) \n",
        "            w_grad = (2/len(x))*(np.dot(err,x)) + self.lamb*sum \n",
        "        elif self.penalty == \"l2\":\n",
        "            sum = 0 \n",
        "            for i in range(len(self.w)):\n",
        "                sum += self.w[i]**2 \n",
        "            w_grad = (2/len(x))*(np.dot(err,x)) + self.lamb*sum \n",
        "        \n",
        "        b_grad = (2/len(x))*np.sum(err) \n",
        "\n",
        "        return w_grad, b_grad\n",
        "\n",
        "    \n",
        "    def initialize_w(self, x):\n",
        "\n",
        "        w_library = {\n",
        "            \"one\":np.ones(x.shape[1]),\n",
        "            \"LeCun\":np.random.uniform(low = -np.sqrt(1.0 / x.shape[1]), high = np.sqrt(1.0 / x.shape[1]), size = x.shape[1]),  \n",
        "            \"random\":np.random.randint(0,1, size = x.shape[1]) \n",
        "        }\n",
        "\n",
        "        return w_library[self.initialize]\n",
        "\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.w = self.initialize_w(x)\n",
        "        self.b = 0\n",
        "        for _ in range(self.max_iter):\n",
        "            z = self.fwpass(x)\n",
        "            err = -(y - z) \n",
        "            w_grad, b_grad = self.bwpass(x, err)\n",
        "\n",
        "            self.w = self.w - self.lr*w_grad \n",
        "\n",
        "\n",
        "            self.b = self.b - self.lr*self.b \n",
        "\n",
        "        return self.w, self.b\n",
        "\n",
        "#csv에 넣기 위한 부분\n",
        "    def predict(self, x):\n",
        "\n",
        "        z = self.fwpass(x)\n",
        "        for i in range(len(z)):\n",
        "            if z[i] > 0.5: \n",
        "                z[i] = 1\n",
        "            else :\n",
        "                z[i] = 0\n",
        "        return z\n",
        "\n",
        "#csv 파일 만들기 위해 추가된 부분\n",
        "    def make_sub(self, x):\n",
        "        z = self.predict(x)\n",
        "        z = z.astype(np.int64)\n",
        "        sub_df = pd.DataFrame({'label' : z})\n",
        "        sub_df.index = range(0,z.shape[0])\n",
        "        sub_df.to_csv('submission_example.csv', index_label=['id'])\n",
        "\n",
        "    def score(self, x, y):\n",
        "\n",
        "        return np.mean(self.predict(x) == y)\n",
        "    \n",
        "    \n",
        "    def feature_importance(self, coef, column_to_use):\n",
        "\n",
        "        w = self.w.copy()\n",
        "        for i in range(len(self.w)): \n",
        "            w[i] = abs(self.w[i]) \n",
        "        dic = {} \n",
        "        for i in range(len(w)):\n",
        "            dic[w[i]] = column_to_use[i]\n",
        "        s = w.argsort() \n",
        "        w = w[s]\n",
        "        self.w = self.w[s]   \n",
        "        rank_li = [] \n",
        "        for i in range(len(column_to_use)):\n",
        "            rank_li.append(((len(w)-(i+1)), [dic[w[i]], self.w[i]])) \n",
        "        rank_li.reverse()\n",
        "        return rank_li"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IumJ7KTKEP6w"
      },
      "outputs": [],
      "source": [
        "#피처로 사용할 컬럼의 수.\n",
        "column_to_use = ['WC','Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'Dic',\n",
        "       'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they',\n",
        "       'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate',\n",
        "       'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect',\n",
        "       'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend',\n",
        "       'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat',\n",
        "       'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body',\n",
        "       'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve',\n",
        "       'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture',\n",
        "       'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home',\n",
        "       'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent',\n",
        "       'nonflu', 'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC',\n",
        "       'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP']\n",
        "train_x = data[column_to_use].copy()\n",
        "train_y = data['label'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "x6hjC_slPEdv",
        "outputId": "a7c302a9-f43e-4952-c0b1-4b50d383ee2b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.metrics import accuracy_score\\nimport time\\n\\nss=StandardScaler()\\n#ss.fit(train_x)\\n#train_x = ss.transform(train_x)\\n\\nkfold = KFold(n_splits=5, shuffle = True)\\ncv_accuracy = []\\nn_iter = 0\\n\\nlr = LogisticRegression(max_iter=700, penalty=\"l2\", initialize = \"LeCun\")\\nstart = time.time()\\n\\nfor train_index, test_index in kfold.split(data):  \\n    x_train, x_test = train_x.iloc[train_index], train_x.iloc[test_index]\\n    y_train, y_test = train_y.iloc[train_index], train_y.iloc[test_index]\\n\\n    x_train=ss.fit_transform(x_train)\\n    x_test=ss.transform(x_test) \\n\\n    coef, interrupt = lr.fit(x_train, y_train)\\n    pred = lr.predict(x_test)\\n    n_iter += 1\\n\\n    accuracy = np.round(accuracy_score(y_test, pred), 4) \\n    train_size = x_train.shape[0]\\n    test_size = x_test.shape[0]\\n    print(\\'\\n#{0} 교차 검증 정확도 : {1},  학습 데이터 크기 : {2},  검증 데이터 크기 : {3}\\'\\n          .format(n_iter, accuracy, train_size, test_size))\\n    print(\\'#{0} 검증 세트 인덱스 : {1}\\'.format(n_iter,test_index))\\n    cv_accuracy.append(accuracy)\\nprint(\\'\\n## 평균 검증 정확도:\\', np.mean(cv_accuracy))\\n#shuffle 있으면 0.74 / 없으면 0.75\\n#람다 1일때, 100 72, 700 74, 900 74\\n#람다 2 / 300 73 500 74  700 72 900 67\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#비교분석 위해 사용한 k폴드. 하지만 피처랑 파라미터의 잘못인지 정확도가 더 낮게 나옴. / reject함.\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "ss=StandardScaler()\n",
        "#ss.fit(train_x)\n",
        "#train_x = ss.transform(train_x)\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle = True)\n",
        "cv_accuracy = []\n",
        "n_iter = 0\n",
        "\n",
        "lr = LogisticRegression(max_iter=700, penalty=\"l2\", initialize = \"LeCun\")\n",
        "start = time.time()\n",
        "\n",
        "for train_index, test_index in kfold.split(data):  \n",
        "    x_train, x_test = train_x.iloc[train_index], train_x.iloc[test_index]\n",
        "    y_train, y_test = train_y.iloc[train_index], train_y.iloc[test_index]\n",
        "\n",
        "    x_train=ss.fit_transform(x_train)\n",
        "    x_test=ss.transform(x_test) \n",
        "\n",
        "    coef, interrupt = lr.fit(x_train, y_train)\n",
        "    pred = lr.predict(x_test)\n",
        "    n_iter += 1\n",
        "\n",
        "    accuracy = np.round(accuracy_score(y_test, pred), 4) \n",
        "    train_size = x_train.shape[0]\n",
        "    test_size = x_test.shape[0]\n",
        "    print('\\n#{0} 교차 검증 정확도 : {1},  학습 데이터 크기 : {2},  검증 데이터 크기 : {3}'\n",
        "          .format(n_iter, accuracy, train_size, test_size))\n",
        "    print('#{0} 검증 세트 인덱스 : {1}'.format(n_iter,test_index))\n",
        "    cv_accuracy.append(accuracy)\n",
        "print('\\n## 평균 검증 정확도:', np.mean(cv_accuracy))\n",
        "#shuffle 있으면 0.74 / 없으면 0.75\n",
        "#람다 1일때, 100 72, 700 74, 900 74\n",
        "#람다 2 / 300 73 500 74  700 72 900 67\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "192Ax9f6xWtG",
        "outputId": "892cefbd-6f72-4448-baf8-6126b947d5f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0.5095615386962891\n",
            "Accuray: 0.8206106870229007\n"
          ]
        }
      ],
      "source": [
        "#테스트 데이터 정확도 82가 나옴. 구현부 부분. 실제 정확도랑은 꽤 차이가 있음. 과적합 발생 한것으로 보임 \n",
        "import time\n",
        "from  sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, random_state = 11)\n",
        "ss=StandardScaler()\n",
        "train_x=ss.fit_transform(train_x)\n",
        "test_x=ss.transform(test_x) \n",
        "\n",
        "lr = LogisticRegression(max_iter=300, penalty=\"l2\", initialize = \"LeCun\")\n",
        "start = time.time()\n",
        "coef, interrupt = lr.fit(train_x, train_y)\n",
        "print(f'time: {time.time() - start}')\n",
        "print(f\"Accuray: {lr.score(test_x, test_y)}\")\n",
        "\n",
        "# 100 81 / 람다 0.2일때  0.809\n",
        "# 300 81 / 0.82\n",
        "# 500 82 / 0.80\n",
        "# 700 81 / 0.79\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgjjE3eZH4Rz",
        "outputId": "4c799d8c-5664-4f78-c2a5-0a7011919f69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, ['achieve', -0.30023278342867293]),\n",
              " (1, ['swear', -0.27632802334296047]),\n",
              " (2, ['work', -0.22669933849097382]),\n",
              " (3, ['cause', -0.22626673383401144]),\n",
              " (4, ['feel', 0.22425605420760036]),\n",
              " (5, ['ingest', -0.21733114284452773]),\n",
              " (6, ['OtherP', -0.20800481578614358]),\n",
              " (7, ['interrog', -0.19738669680765802]),\n",
              " (8, ['article', -0.19475860781118168]),\n",
              " (9, ['netspeak', 0.18239923317887333]),\n",
              " (10, ['negate', -0.1806686669125388]),\n",
              " (11, ['Exclam', 0.174008172110954]),\n",
              " (12, ['percept', 0.16593638499019858]),\n",
              " (13, ['anger', -0.16539838809639498]),\n",
              " (14, ['Sixltr', -0.15745640118923804]),\n",
              " (15, ['health', 0.1423573367655718]),\n",
              " (16, ['anx', 0.1355303289049337]),\n",
              " (17, ['bio', 0.13283222246272305]),\n",
              " (18, ['affect', 0.12415924314087332]),\n",
              " (19, ['time', -0.1230042910675488]),\n",
              " (20, ['Period', -0.11882171612564317]),\n",
              " (21, ['quant', -0.11831058531660578]),\n",
              " (22, ['focusfuture', -0.11669344847912337]),\n",
              " (23, ['Authentic', 0.11167941590734333]),\n",
              " (24, ['ppron', 0.11039931029268903]),\n",
              " (25, ['Quote', -0.10968995170255073]),\n",
              " (26, ['Tone', 0.10819926492887641]),\n",
              " (27, ['function', -0.09956976088332851]),\n",
              " (28, ['female', -0.09672668574876994]),\n",
              " (29, ['differ', -0.09538650998145547]),\n",
              " (30, ['male', -0.09358329052677014]),\n",
              " (31, ['we', -0.09205607422849342]),\n",
              " (32, ['pronoun', 0.09172518443185508]),\n",
              " (33, ['tentat', -0.09090552146628444]),\n",
              " (34, ['WC', 0.0888456645235452]),\n",
              " (35, ['number', -0.08841434424681327]),\n",
              " (36, ['posemo', 0.087462843889858]),\n",
              " (37, ['friend', 0.08416016406125859]),\n",
              " (38, ['sexual', -0.08086286011314475]),\n",
              " (39, ['money', -0.07860637504130777]),\n",
              " (40, ['assent', -0.0734920441961221]),\n",
              " (41, ['Apostro', 0.07194456516082218]),\n",
              " (42, ['shehe', 0.06878515085354427]),\n",
              " (43, ['space', 0.05373917119055561]),\n",
              " (44, ['cogproc', -0.05336709087787248]),\n",
              " (45, ['WPS', 0.049944041112090214]),\n",
              " (46, ['leisure', -0.048232875421213156]),\n",
              " (47, ['SemiC', -0.047734047175577374]),\n",
              " (48, ['QMark', -0.04739356325629915]),\n",
              " (49, ['ipron', -0.04711311706988096]),\n",
              " (50, ['reward', -0.044818933372492244]),\n",
              " (51, ['body', -0.04466427782070163]),\n",
              " (52, ['informal', -0.04242877369706328]),\n",
              " (53, ['drives', 0.04146493027137472]),\n",
              " (54, ['they', -0.04130054544474183]),\n",
              " (55, ['Analytic', -0.04035806031140209]),\n",
              " (56, ['prep', 0.0399150185015366]),\n",
              " (57, ['affiliation', -0.0390324059610796]),\n",
              " (58, ['discrep', -0.0388158533603017]),\n",
              " (59, ['certain', -0.036966679770296475]),\n",
              " (60, ['focuspast', 0.03467827637892434]),\n",
              " (61, ['AllPunc', -0.030734543412550837]),\n",
              " (62, ['conj', 0.02965265298061368]),\n",
              " (63, ['sad', 0.02893303004890868]),\n",
              " (64, ['negemo', -0.02712584446292095]),\n",
              " (65, ['motion', -0.023612978322974272]),\n",
              " (66, ['nonflu', -0.022802197288309005]),\n",
              " (67, ['Parenth', -0.020652467515475618]),\n",
              " (68, ['Comma', 0.018923252324600237]),\n",
              " (69, ['home', 0.018046868013131757]),\n",
              " (70, ['you', -0.016275197787258215]),\n",
              " (71, ['relig', 0.015935641989424324]),\n",
              " (72, ['focuspresent', 0.014361714133332985]),\n",
              " (73, ['Clout', 0.012436656209065443]),\n",
              " (74, ['social', 0.012332632699874363]),\n",
              " (75, ['relativ', 0.0122209159345809]),\n",
              " (76, ['risk', -0.012134482474772593]),\n",
              " (77, ['adverb', 0.010751258812149929]),\n",
              " (78, ['i', 0.01043384150841649]),\n",
              " (79, ['verb', -0.009491480019352199]),\n",
              " (80, ['compare', -0.00838271464745508]),\n",
              " (81, ['Dash', 0.008304555331385749]),\n",
              " (82, ['Dic', 0.008168711889310976]),\n",
              " (83, ['family', 0.0069952993650972866]),\n",
              " (84, ['filler', 0.006827092000027827]),\n",
              " (85, ['see', 0.006711194031195935]),\n",
              " (86, ['power', -0.00621591137194679]),\n",
              " (87, ['auxverb', -0.006198343848357892]),\n",
              " (88, ['hear', -0.005952107662838473]),\n",
              " (89, ['adj', 0.004321811561673403]),\n",
              " (90, ['death', -0.00422633972253184]),\n",
              " (91, ['Colon', -0.0009158952613685965]),\n",
              " (92, ['insight', 0.0004259271310301174])]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#피처 상관관계\n",
        "lr.feature_importance(coef, column_to_use)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "o54t6PaNRrNV"
      },
      "outputs": [],
      "source": [
        "test_data = test[column_to_use].copy()\n",
        "test_data = ss.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BDHx8BBTUKG",
        "outputId": "c60b4361-de13-45c6-c107-d4e438dd7cc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0., 1., ..., 0., 0., 0.])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr.predict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cysQgDH5Uzyi"
      },
      "outputs": [],
      "source": [
        "lr.make_sub(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqX9P_6Cy5ea",
        "outputId": "ab2a54f8-2ec6-487b-c316-8283b59d410c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r  0% 0.00/10.8k [00:00<?, ?B/s]\r100% 10.8k/10.8k [00:00<00:00, 57.3kB/s]\n",
            "Successfully submitted to [SKKU 2023-1 Machine Learning] Second Project"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c skku-2023-1-machine-learning-second-project -f submission_example.csv -m \"Message\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
